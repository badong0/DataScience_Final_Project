{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85263ab5",
      "metadata": {},
      "source": [
        "# Phase 1 & 2: Data Loading, Cleaning, and Preprocessing\n",
        "\n",
        "This notebook covers:\n",
        "- **Phase 1**: Environment setup and data inspection\n",
        "- **Phase 2**: Data cleaning, preprocessing, and feature selection for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638a5436",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e782fd",
      "metadata": {},
      "source": [
        "## Phase 1.2: Load and Inspect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae75104a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data/Data Science Dataset - DATABASE.csv')\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b58d057",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cc9493",
      "metadata": {},
      "source": [
        "## Phase 2.1: Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03b1203",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with missing dates (per methodology)\n",
        "print(f\"Rows before dropping missing dates: {len(df)}\")\n",
        "df = df.dropna(subset=['Date'])\n",
        "print(f\"Rows after dropping missing dates: {len(df)}\")\n",
        "\n",
        "# Check for rows with all NaN values (completely empty rows)\n",
        "empty_rows = df[df.isnull().all(axis=1)]\n",
        "print(f\"Completely empty rows: {len(empty_rows)}\")\n",
        "if len(empty_rows) > 0:\n",
        "    df = df.dropna(how='all')\n",
        "    print(f\"Rows after dropping completely empty rows: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b7679d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric columns for imputation\n",
        "numeric_columns = ['Sleep_Hours', 'Work_Hours', 'Study_Hours', 'Chore_Time_Mins', \n",
        "                   'Distraction_Time_Mins', 'Travel Time (Hours)', 'Music_Time_Hours', \n",
        "                   'Tasks_Completed', 'Mood_Rating', 'Focus_Rating']\n",
        "\n",
        "# Log missing values before imputation\n",
        "missing_before = df[numeric_columns].isnull().sum()\n",
        "print(\"Missing values before imputation:\")\n",
        "print(missing_before[missing_before > 0])\n",
        "\n",
        "# Impute missing numeric values with 0 (per methodology)\n",
        "df[numeric_columns] = df[numeric_columns].fillna(0)\n",
        "\n",
        "# Verify imputation\n",
        "missing_after = df[numeric_columns].isnull().sum()\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(missing_after[missing_after > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8ec75a",
      "metadata": {},
      "source": [
        "## Phase 2.2: Parse and Validate Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8f9e5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse Date column to datetime\n",
        "# Format appears to be MM-DD-YY\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')\n",
        "\n",
        "# Validate Day of the Week against computed weekday\n",
        "df['Computed_Day'] = df['Date'].dt.day_name()\n",
        "df['Day_Match'] = df['Day of the Week'] == df['Computed_Day']\n",
        "\n",
        "# Check for mismatches\n",
        "mismatches = df[~df['Day_Match']]\n",
        "if len(mismatches) > 0:\n",
        "    print(f\"Found {len(mismatches)} date/day mismatches:\")\n",
        "    print(mismatches[['Date', 'Day of the Week', 'Computed_Day']])\n",
        "else:\n",
        "    print(\"All dates match their day of the week!\")\n",
        "\n",
        "# Add Is_Weekend column for \"Weekend Bleed\" hypothesis\n",
        "df['Is_Weekend'] = df['Date'].dt.dayofweek.isin([5, 6])  # Saturday=5, Sunday=6\n",
        "\n",
        "print(f\"\\nWeekend days: {df['Is_Weekend'].sum()}\")\n",
        "print(f\"Weekday days: {(~df['Is_Weekend']).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63823c1c",
      "metadata": {},
      "source": [
        "## Phase 2.3: Handle Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13440da3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore categorical variables\n",
        "print(\"Mode of Transport unique values:\")\n",
        "print(df['Mode of Transport'].value_counts())\n",
        "print(\"\\nMain_Music_Genre unique values:\")\n",
        "print(df['Main_Music_Genre'].value_counts())\n",
        "\n",
        "# Note: Per methodology, we'll exclude categorical variables from clustering\n",
        "# but keep them for exploratory analysis and hypothesis testing\n",
        "# Optionally, we can one-hot encode them if needed later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b704064",
      "metadata": {},
      "source": [
        "## Phase 2.4: Feature Selection for Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d346c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for clustering based on methodology\n",
        "# Inputs: Sleep_Hours, Music_Time_Hours, Travel Time (Hours)\n",
        "# Behaviors: Work_Hours, Study_Hours, Chore_Time_Mins, Distraction_Time_Mins, Tasks_Completed\n",
        "\n",
        "clustering_features = [\n",
        "    'Sleep_Hours',           # Input\n",
        "    'Music_Time_Hours',      # Input\n",
        "    'Travel Time (Hours)',   # Input\n",
        "    'Work_Hours',            # Behavior\n",
        "    'Study_Hours',           # Behavior\n",
        "    'Chore_Time_Mins',       # Behavior\n",
        "    'Distraction_Time_Mins', # Behavior\n",
        "    'Tasks_Completed'        # Behavior\n",
        "]\n",
        "\n",
        "# Create feature matrix for clustering\n",
        "X = df[clustering_features].copy()\n",
        "\n",
        "# Targets (excluded from clustering, used for validation)\n",
        "targets = ['Mood_Rating', 'Focus_Rating']\n",
        "\n",
        "print(\"Features selected for clustering:\")\n",
        "print(clustering_features)\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(\"\\nFeature statistics:\")\n",
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cded9c30",
      "metadata": {},
      "source": [
        "## Phase 2.5: Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd59d7cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply StandardScaler (Z-score normalization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=clustering_features, index=X.index)\n",
        "\n",
        "print(\"Scaled feature matrix statistics:\")\n",
        "print(X_scaled_df.describe())\n",
        "print(\"\\nScaled feature means (should be ~0):\")\n",
        "print(X_scaled_df.mean())\n",
        "print(\"\\nScaled feature std (should be ~1):\")\n",
        "print(X_scaled_df.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb227efb",
      "metadata": {},
      "source": [
        "## Save Processed Data\n",
        "\n",
        "Save the cleaned and scaled data for use in subsequent notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5507300",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data\n",
        "# Save cleaned dataframe\n",
        "df.to_csv('data/cleaned_data.csv', index=False)\n",
        "\n",
        "# Save scaled features\n",
        "X_scaled_df.to_csv('data/scaled_features.csv', index=False)\n",
        "\n",
        "# Save feature names and targets for reference\n",
        "import json\n",
        "metadata = {\n",
        "    'clustering_features': clustering_features,\n",
        "    'targets': targets,\n",
        "    'n_samples': len(df)\n",
        "}\n",
        "with open('data/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Processed data saved successfully!\")\n",
        "print(f\"- Cleaned data: data/cleaned_data.csv ({len(df)} rows)\")\n",
        "print(f\"- Scaled features: data/scaled_features.csv\")\n",
        "print(f\"- Metadata: data/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066200fa",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
